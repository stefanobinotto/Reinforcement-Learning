{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Jack's Car Rental\n",
    "\n",
    "The code below generates Figure 4.4, which can be found in Chapter 4 (Dynamic Programming) of \"Reinforcement Learning: An Introduction\" by Andrew Barto and Richard S. Sutton.\n",
    "The main goal of this lab is to help the understanding of the concepts in Dynamic Programming through a concrete example, given by Jack's Car Rental problem. In particular we will see how the value-function can be computed and how it allows us to better understand the environment, to take meaningful actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main features of the environment\n",
    "- 2 locations managed by Jack for a nationwide car rental company\n",
    "- Customers arrive with an unknown distribution\n",
    "- If there is a car at the place and time the customer arrives, $10\\$$ are earned by renting it\n",
    "- Jack can move the cars overnight, paying $2\\$$ for each car\n",
    "- Cars requested and returned are modelled either through a Poisson random variable (or through a constant value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# 2017 Aja Rangaswamy (aja004@gmail.com)                              #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded parameters for the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum # of cars in each location\n",
    "MAX_CARS = 20\n",
    "\n",
    "# maximum # of cars to move during night\n",
    "MAX_MOVE_OF_CARS = 5\n",
    "\n",
    "# Expectations are used to simulate the environment!\n",
    "# expectation for rental requests in first location\n",
    "RENTAL_REQUEST_FIRST_LOC = 3\n",
    "\n",
    "# expectation for rental requests in second location\n",
    "RENTAL_REQUEST_SECOND_LOC = 4\n",
    "\n",
    "# expectation for # of cars returned in first location\n",
    "RETURNS_FIRST_LOC = 3\n",
    "\n",
    "# expectation for # of cars returned in second location\n",
    "RETURNS_SECOND_LOC = 2\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# credit earned by a car\n",
    "RENTAL_CREDIT = 10\n",
    "\n",
    "# cost of moving a car\n",
    "MOVE_CAR_COST = 2\n",
    "\n",
    "# all possible actions\n",
    "actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)\n",
    "# Moving a negative number of cars means that we are moving them in the opposite direction (from II to I)\n",
    "\n",
    "# An upper bound for poisson distribution\n",
    "# If n is greater than this value, then the probability of getting n is truncated to 0\n",
    "POISSON_UPPER_BOUND = 11\n",
    "# This is done to save some computational time, since probability at this point is negligible\n",
    "\n",
    "# Probability for poisson distribution\n",
    "# @lam: lambda should be less than 10 for this function\n",
    "poisson_cache = dict()\n",
    "# We will iteratively build a dictionary for the poisson random variable values, again to save computational time\n",
    "# The constraint on lambda is only due to \"bad\" indexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the value of the discrete poisson distribution with a given mean (```lam```) for a particular value (```n```) and then update the dictionary. If we already computed that value in a previous iteration, we just pick it from the dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_probability(n, lam):\n",
    "    global poisson_cache\n",
    "    # Simple identifier if lam < 10\n",
    "    key = n * 10 + lam\n",
    "    if key not in poisson_cache:\n",
    "        poisson_cache[key] = poisson.pmf(n, lam)\n",
    "    return poisson_cache[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is the key component in the computation of the value function. Recall that in Dynamic Programming, the environment's transitions are known, or at least they are characterized with a probabilistic model. The probability here is due only to the intrinsic randomness of the environment, therefore we have a perfect knowledge of the consequence of each action.\n",
    "This knowledge allows us to compute the expected reward as a function of the current state and action.\n",
    "\n",
    "In particular, we are computing the only quantity that is required to compute the value function for a fixed policy through the Bellman equation:\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}[r(s, \\pi(s)) + V^\\pi(s')]$,\n",
    "\n",
    "or to perform the policy iteration algorithm, that is what will actually use in order to compute the optimal value-function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_return(state, action, state_value, constant_returned_cars):\n",
    "    \"\"\"\n",
    "    @state: [# of cars in first location, # of cars in second location]\n",
    "    @action: positive if moving cars from first location to second location,\n",
    "            negative if moving cars from second location to first location\n",
    "    @stateValue: state value matrix\n",
    "    @constant_returned_cars:  if set True, model is simplified such that\n",
    "    the # of cars returned in daytime becomes constant\n",
    "    rather than a random value from poisson distribution, which will reduce calculation time\n",
    "    and leave the optimal policy/value state matrix almost the same\n",
    "    \"\"\"\n",
    "    # initialize total return\n",
    "    returns = 0.0\n",
    "\n",
    "    # cost for moving cars\n",
    "    returns -= MOVE_CAR_COST * abs(action)          # abs() because the action can also be negative\n",
    "\n",
    "    # moving cars\n",
    "    NUM_OF_CARS_FIRST_LOC = min(state[0] - action, MAX_CARS)\n",
    "    NUM_OF_CARS_SECOND_LOC = min(state[1] + action, MAX_CARS)\n",
    "\n",
    "    # We are computing an expectation in at least 2 dimensions, i.e. for each (meaningful) value we need to compute its probability!\n",
    "    # go through all possible rental requests\n",
    "    for rental_request_first_loc in range(POISSON_UPPER_BOUND):\n",
    "        for rental_request_second_loc in range(POISSON_UPPER_BOUND):\n",
    "            # probability for current combination of rental requests\n",
    "            prob = poisson_probability(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \\\n",
    "                poisson_probability(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC)\n",
    "\n",
    "            num_of_cars_first_loc = NUM_OF_CARS_FIRST_LOC\n",
    "            num_of_cars_second_loc = NUM_OF_CARS_SECOND_LOC\n",
    "\n",
    "            # valid rental requests should be less than actual # of cars\n",
    "            valid_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc)\n",
    "            valid_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc)\n",
    "\n",
    "            # get credits for renting\n",
    "            reward = (valid_rental_first_loc + valid_rental_second_loc) * RENTAL_CREDIT\n",
    "            num_of_cars_first_loc -= valid_rental_first_loc\n",
    "            num_of_cars_second_loc -= valid_rental_second_loc\n",
    "\n",
    "            if constant_returned_cars:\n",
    "                # get returned cars, those cars can be used for renting tomorrow\n",
    "                returned_cars_first_loc = RETURNS_FIRST_LOC\n",
    "                returned_cars_second_loc = RETURNS_SECOND_LOC\n",
    "                num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                #compute state value function/expected return ( quindi Expectation[G_t|S_t=s])\n",
    "                returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc])\n",
    "            else:\n",
    "                for returned_cars_first_loc in range(POISSON_UPPER_BOUND):\n",
    "                    for returned_cars_second_loc in range(POISSON_UPPER_BOUND):\n",
    "                        prob_return = poisson_probability(\n",
    "                            returned_cars_first_loc, RETURNS_FIRST_LOC) * poisson_probability(returned_cars_second_loc, RETURNS_SECOND_LOC)\n",
    "                        num_of_cars_first_loc_ = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                        num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                        prob_ = prob_return * prob\n",
    "                        returns += prob_ * (reward + DISCOUNT *\n",
    "                                            state_value[num_of_cars_first_loc_, num_of_cars_second_loc_])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements the in-place policy iteration algorithm. So starting with a particular policy, the corresponding value-function is considered, which is obtained through multiple sweeps, until convergence. From the new value-function thus obtained, a better policy is found (policy improvement theorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_13932\\1396217813.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  policy = np.zeros(value.shape, dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value change 196.62783361783852\n",
      "max value change 134.98823859766583\n",
      "max value change 91.41415360228919\n"
     ]
    }
   ],
   "source": [
    "# This is the parameter of the function in the original code\n",
    "constant_returned_cars = True\n",
    "\n",
    "# Initialization of the value-function\n",
    "value = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "\n",
    "# We start considering the simplest policy: for every possible state, no car is moved\n",
    "# It makes sense: if is not clear what a meaningful action might be, better not to pay the cost of moving cars!\n",
    "policy = np.zeros(value.shape, dtype=np.int)\n",
    "\n",
    "iterations = 0\n",
    "_, axes = plt.subplots(2, 3, figsize=(40, 20))\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "axes = axes.flatten()\n",
    "while True:\n",
    "    fig = sns.heatmap(np.flipud(policy), cmap=\"YlGnBu\", ax=axes[iterations])\n",
    "    fig.set_ylabel('# cars at first location', fontsize=30)\n",
    "    fig.set_yticks(list(reversed(range(MAX_CARS + 1))))\n",
    "    fig.set_xlabel('# cars at second location', fontsize=30)\n",
    "    fig.set_title('policy {}'.format(iterations), fontsize=30)\n",
    "\n",
    "    # policy evaluation (in-place)\n",
    "    while True:\n",
    "        old_value = value.copy()\n",
    "        # Sweep through all states following the same policy\n",
    "        for i in range(MAX_CARS + 1):\n",
    "            for j in range(MAX_CARS + 1):\n",
    "                new_state_value = expected_return([i, j], policy[i, j], value, constant_returned_cars)\n",
    "                # in-place update!\n",
    "                value[i, j] = new_state_value\n",
    "        max_value_change = abs(old_value - value).max()\n",
    "        print('max value change {}'.format(max_value_change))\n",
    "        # termination condition\n",
    "        if max_value_change < 1e-4:\n",
    "            break\n",
    "\n",
    "    # policy improvement\n",
    "    policy_stable = True\n",
    "    for i in range(MAX_CARS + 1):\n",
    "        for j in range(MAX_CARS + 1):\n",
    "            old_action = policy[i, j]\n",
    "            action_returns = []\n",
    "            for action in actions:\n",
    "                # if it is a 'legal' action, i.e. I am not trying to move more cars than I have in that location\n",
    "                if (0 <= action <= i) or (-j <= action <= 0):\n",
    "                    action_returns.append(expected_return([i, j], action, value, constant_returned_cars))\n",
    "                # fraud against the car rental company ---> lawyers, possible imprisonment, very low reward\n",
    "                else:\n",
    "                    action_returns.append(-np.inf)\n",
    "            # Substitution with greedy policy at all states\n",
    "            new_action = actions[np.argmax(action_returns)]\n",
    "            policy[i, j] = new_action\n",
    "            if policy_stable and old_action != new_action:\n",
    "                policy_stable = False\n",
    "    print('policy stable {}'.format(policy_stable))\n",
    "    # If not stable, back to the previous while, and iterate!\n",
    "\n",
    "    # If stable instead\n",
    "    if policy_stable:\n",
    "        fig = sns.heatmap(np.flipud(value), cmap=\"YlGnBu\", ax=axes[-1])\n",
    "        fig.set_ylabel('# cars at first location', fontsize=30)\n",
    "        fig.set_yticks(list(reversed(range(MAX_CARS + 1))))\n",
    "        fig.set_xlabel('# cars at second location', fontsize=30)\n",
    "        fig.set_title('optimal value', fontsize=30)\n",
    "        break\n",
    "\n",
    "    iterations += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
